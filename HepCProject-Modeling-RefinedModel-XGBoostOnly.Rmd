---
title: "capstone_modeling"
author: "Laura Olson"
date: "May 20, 2017"
output: html_document
---

```{r}
#libraries being used in this file
library(dplyr)
library(ROSE)
library(rpart)
library(Metrics)
library(randomForest)
library(lift)
suppressWarnings(library(dummies))
suppressWarnings(library(xgboost))
library(Rmisc)
require(stats)
require(caret)

#require(corrplot)
#require(Rtsne)
#require(knitr)
#require(ggplot2)
#knitr::opts_chunk$set(cache=TRUE)


```


```{r}
  #Patient variables = 1-5
  #Demographic variables = 6-12
  #Freq HEDIS ICD codes = 13- 68
  #Average HEDIS ICD codes = 69 - 124
  #Binary HEDIS ICD codes = 125 - 180
  #Average CCS ICD codes = 181 - 461
  #Binary CCS ICD codes = 462 - 741
  #Freq HEDIS PROC codes = 742 - 800
  #Average HEDIS PROC codes = 801 - 859
  #Binary HEDIS PROC codes = 860 - 918
  #Totals and Averages on Encounter Type = 919 - 980
  #Insurance = 981 - 982
  #ACS data = 983 - 1024
  #Social = 1025 - 1032
  #Meds = 1033 - 1079


###Import and format columns
#full data set not trimmed
flat_file_0 <- read.csv("Z:/Flat File/patients_all_variables_09-07-2017.csv", ",", header = TRUE, stringsAsFactors = FALSE)

#1month trimmed
flat_file_1 <- read.csv("Z:/Flat File/Date Trimmed 1 month/patients_all_variables_trimmed_1_mo_10-17-2017.csv", ",", header = TRUE, stringsAsFactors = FALSE)

#6 month trimmed
flat_file_6 <- read.csv("Z:/Flat File/Date Trimmed 6 months/patients_all_variables_trimmed_6_mo_10-18-2017.csv", ",", header = TRUE, stringsAsFactors = FALSE)

#12 month trimmed
flat_file_12 <- read.csv("Z:/Flat File/Date Trimmed 12 months/patients_all_variables_trimmed_12_mo_10-30-2017.csv", ",", header = TRUE, stringsAsFactors = FALSE)

#specify which file to use for this run
flat_file<-flat_file_12

```




```{r}

#reformat some of the variables to numeric or character or factor

flat_file$PATIENT_ID <- as.character(flat_file$PATIENT_ID)
flat_file[,2:3] <- lapply(flat_file[,2:3], as.Date)
flat_file$HCV_STATUS <- as.numeric(flat_file$HCV_STATUS)
flat_file[,6:12] <- lapply(flat_file[,6:12], as.factor)
flat_file[,13:980] <- lapply(flat_file[,13:980], as.numeric)
flat_file[,981:982] <- lapply(flat_file[,981:982], as.factor)
flat_file[,983:1024] <- lapply(flat_file[,983:1024], as.numeric)
flat_file$TOBACCO_USER_STATUS_DER <- as.factor(flat_file$TOBACCO_USER_STATUS_DER)
flat_file[,1026:1030] <- lapply(flat_file[,1026:1030], as.numeric)
flat_file$SEX_ENC_TYPE  <- as.factor(flat_file$SEX_ENC_TYPE )
flat_file[,1032:1079] <- lapply(flat_file[,1032:1079], as.numeric)
sum(is.na(flat_file))   

##bring in Cindy's correct generational cohort variable recalculated later after the flat file was generated
demo_flat_file <- read.csv("Z:/Flat File/pts_demo.csv", ",", header = TRUE, stringsAsFactors = FALSE)
demo_flat_file$PATIENT_ID <- as.character(demo_flat_file$PATIENT_ID)
demo_flat_file$GENERATION_CATEGORY<-as.factor(demo_flat_file$GENERATION_CATEGORY)
demo_flat_file<-demo_flat_file[,c(1,8)]

flat_file_2<-inner_join(flat_file,demo_flat_file, by = "PATIENT_ID")
flat_file_2$GENERATION_CATEGORY.x<-flat_file_2$GENERATION_CATEGORY.y
names(flat_file_2$GENERATION_CATEGORY.x)<-paste("GENERATION_CATEGORY")
flat_file<-flat_file_2[,1:1079]
```

```{r}

##REFINED MODEL VARIABLES###
#select expanded model variables only
model_variables <- flat_file[c(5:12, 69:124, 181:461, 801:859, 920, 922, 924, 925, 927, 929:1079)]
dim(model_variables)
patients <- model_variables

dim(patients)
## Code to remove duplicating and correlated variables - Remove Average_ICD varialbes and leave Average_ICD_CCS
patients<-patients[,-c(65:345)]
dim(patients)

###cpt variables logic to combine varibles
patients[,(ncol(patients)+1)]<-patients$AVERAGE_PROC_76826+patients$AVERAGE_PROC_76828
names(patients)[ncol(patients)]<-paste("AVE_COMB_ECHOCARDIOGRAPHY")

patients[,(ncol(patients)+1)]<-patients$AVERAGE_PROC_86631+patients$AVERAGE_PROC_86632
names(patients)[ncol(patients)]<-paste("AVE_COMB_CHLAMYDIA")

patients[,(ncol(patients)+1)]<-patients$AVERAGE_PROC_76813+patients$AVERAGE_PROC_84163
names(patients)[ncol(patients)]<-paste("AVE_COMB_PREGNANCY_REL")

patients[,(ncol(patients)+1)]<-patients$AVERAGE_PROC_59000+patients$AVERAGE_PROC_76946
names(patients)[ncol(patients)]<-paste("AVE_COMB_AMNIOCENTESIS")

dim(patients)

## remove component variables for the ones combined above
patients<-patients[,-c(grep("AVERAGE_PROC_76826", colnames(patients)),
  grep("AVERAGE_PROC_76828", colnames(patients)),
  grep("AVERAGE_PROC_86631", colnames(patients)),
  grep("AVERAGE_PROC_86632", colnames(patients)),
  grep("AVERAGE_PROC_76813", colnames(patients)),
  grep("AVERAGE_PROC_84163", colnames(patients)),
  grep("AVERAGE_PROC_59000", colnames(patients)),
  grep("AVERAGE_PROC_76946", colnames(patients))
)]
dim(patients)


###encounter tab variables logic to combine
patients[,(ncol(patients)+1)]<-patients$AVERAGE_ENC_TYPE_52+patients$AVERAGE_ENC_TYPE_53
names(patients)[ncol(patients)]<-paste("AVE_COMB_ANESTHESIA")

dim(patients)

##remove combined variables from encounters table
patients<-patients[,-c(grep("AVERAGE_ENC_TYPE_52", colnames(patients)),
  grep("AVERAGE_ENC_TYPE_53", colnames(patients)),
  grep("Avg_Num_Billable_Enc_yr", colnames(patients)),
  grep("Avg_Num_Outpt_Encounters_yr", colnames(patients)),
  grep("AVERAGE_ENC_TYPE_109", colnames(patients)),
  grep("Avg_Num_Inpt_Admits_yr", colnames(patients)),
  grep("AVERAGE_ENC_TYPE_50", colnames(patients)),
  grep("AVERAGE_ENC_TYPE_111", colnames(patients)),
  grep("AVERAGE_ENC_TYPE_3", colnames(patients)),
  grep("Avg_Num_ED_Visits_yr", colnames(patients)),
  grep("Avg_Num_ED_Encounters_yr", colnames(patients)),
  grep("AVERAGE_ENC_TYPE_51", colnames(patients)),
  grep("AVERAGE_ENC_TYPE_53", colnames(patients))
)]

dim(patients)


##logic to remove variables from the ACS table
patients<-patients[,-c(grep("Pct_Non_Hispanic", colnames(patients)),
  grep("Pct_HS_or_More", colnames(patients)),
  grep("Pct_Pop_Female", colnames(patients)),
  grep("Pct_White_Alone", colnames(patients)),
  grep("Median_Age_Female", colnames(patients)),
  grep("Pct_NeveR_Married", colnames(patients)),
  grep("Pct_Masters_or_More", colnames(patients)),
  grep("Median_Age_Male", colnames(patients)),
  grep("Pct_Some_College_or_More", colnames(patients)),
  grep("Pct_Bach_or_More", colnames(patients))
)]

dim(patients)


#get column index
#grep("AVERAGE_ICD_0042", colnames(patients))

```



```{r}
##### BUILD SAMPLED DATASETS

set.seed(12345)

### build train data set to use with different sampling techniques
#separate positive and negative patients into separate tables
patients_negative_imb<-subset(patients,patients$HCV_STATUS==0)
patients_positive_imb<-subset(patients,patients$HCV_STATUS==1)

#randomly select 70% of the negative patients for the train set
train_negative_imb<-sample_frac(patients_negative_imb, 0.7)
nrow(train_negative_imb)

#randomly select 70% of the positive patients for the train set
train_positive_imb<-sample_frac(patients_positive_imb, 0.7)
nrow(train_positive_imb)

## build test data set
#select the remainder of the negaive patients that are not part of the training set to be part of the test set
test_negative_imb<-anti_join(patients_negative_imb,train_negative_imb)
nrow(test_negative_imb)
#select the remainder of the positive patients that are not part of the training set to be part of the test set
test_positive_imb<-anti_join(patients_positive_imb,train_positive_imb)
nrow(test_positive_imb)

### bind the two tables (positive and negative for test and train)
train_patients_imb<-rbind(train_negative_imb,train_positive_imb)
test_patients_imb<-rbind(test_negative_imb,test_positive_imb)

train_patients_vars_imb<-train_patients_imb[,1:ncol(train_patients_imb)]
test_patients_vars_imb<-test_patients_imb[,1:ncol(test_patients_imb)]

dim(train_patients_vars_imb)

#check table
table(train_patients_vars_imb$HCV_STATUS)

#check classes distribution
prop.table(table(train_patients_vars_imb$HCV_STATUS))
#The positive patients are less than 1% of the sample

## original imbalanced data sets: 
#train_patients_vars_imb
#test_patients_vars_imb

##IMBALANCED DATA SETS SAMPLING TECHNIQUES

##1. MANUAL SAMPLING
### build train data set
#separate positive and negative patients into two separate tables
patients_negative<-subset(patients,patients$HCV_STATUS==0)
patients_positive<-subset(patients,patients$HCV_STATUS==1)

#select 70% of the negative patients to be part of the training set
train_negative<-sample_frac(patients_negative, 0.7)
nrow(train_negative)

#select 70% of the positive patients to be part of the training set
train_positive<-sample_frac(patients_positive, 0.7)
nrow(train_positive)
#repeat the positive patients 50 times to balance the imbalanced positives in the sample
train_positive<-train_positive[rep(1:nrow(train_positive),each=50),] 
nrow(train_positive)

 ## build test data set
#select the negative patients who are not part of the training set to be part of the testing set
test_negative<-anti_join(patients_negative,train_negative)
nrow(test_negative)

#select the positive patients who are not part of the training set to be part of the test set
test_positive<-anti_join(patients_positive,train_positive)
nrow(test_positive)
test_positive<-test_positive[rep(1:nrow(test_positive),each=50),] 
nrow(test_positive)

### bind the two tables (positive and negative for test and train)
train_patients<-rbind(train_negative,train_positive)
table(train_patients$HCV_STATUS)
prop.table(table(train_patients$HCV_STATUS))

test_patients<-rbind(test_negative,test_positive)
table(test_patients$HCV_STATUS)
prop.table(table(test_patients$HCV_STATUS))

#remove variables that won't be used in the model
train_patients_vars<-train_patients[,1:ncol(train_patients)]
test_patients_vars<-test_patients[,1:ncol(test_patients)]


# Use the rose package for different sampling techniques

#2. OVER-SAMPLING TECHNIQUE
#oversample the positive patients until the number of entries in the train dataset is 350K
data_balanced_over <- ovun.sample(HCV_STATUS ~ ., data = train_patients_vars_imb, method = "over",N = 350000)$data
table(data_balanced_over$HCV_STATUS)

#oversample the positive patients until the number of entries in the test dataset is 150K. To maintain 70/30 ratio of train to test
test_balanced_over <- ovun.sample(HCV_STATUS ~ ., data = test_patients_vars_imb, method = "over",N = 150000)$data
table(test_balanced_over$HCV_STATUS)

#make the number of items in the samples the same so the data set is balanced

#In the code above, method over instructs the algorithm to perform over sampling. N refers to number of observations in the resulting balanced set. 

#3. UNDER-SAMPLING TECHNIQUE
#Similarly, we can perform undersampling as well. Remember, undersampling is done without replacement.
data_balanced_under <- ovun.sample(HCV_STATUS ~ ., data = train_patients_vars_imb, method = "under", N = 3500, seed = 1)$data
table(data_balanced_under$HCV_STATUS)

#To maintain 70/30 ratio of train to test
test_balanced_under <- ovun.sample(HCV_STATUS ~ ., data = test_patients_vars_imb, method = "under", N = 1500, seed = 1)$data
table(test_balanced_under$HCV_STATUS)


#4. BOTH UNDER AND OVER-SAMPLE
#Now the data set is balanced. But, you see that we've lost significant information from the sample. Let's do both undersampling and oversampling on this imbalanced data. This can be achieved using method = "both". In this case, the minority class is oversampled with replacement and majority class is undersampled without replacement.
data_balanced_both <- ovun.sample(HCV_STATUS ~ ., data = train_patients_vars_imb, method = "both", p=0.5,N=100000, seed = 1)$data
table(data_balanced_both$HCV_STATUS)

#To maintain 70/30 ratio of train to test
test_balanced_both <- ovun.sample(HCV_STATUS ~ ., data = test_patients_vars_imb, method = "both", p=0.5,N=33000, seed = 1)$data
table(test_balanced_both$HCV_STATUS)

#p refers to the probability of positive class in newly generated sample.The data generated from oversampling have expected amount of repeated observations. Data generated from undersampling is deprived of important information from the original data. This leads to inaccuracies in the resulting performance. To encounter these issues, ROSE helps us to generate data synthetically as well. The data generated using ROSE is considered to provide better estimate of original data.

#5. START ROSE SAMPLING
#samples so that the number of positive and negative observations is about the same.
#data.rose <- ROSE(HCV_STATUS ~ ., data = train_patients_vars_imb, seed = 1)$data
#table(data.rose$HCV_STATUS)

#the ratio of train/test about 70/30
#test.rose <- ROSE(HCV_STATUS ~ ., data = test_patients_vars_imb, seed = 1)$data
#table(test.rose$HCV_STATUS)


```


```{r}
#xgboost preprocessing - determine the variance of each variable and eliminate the ones that have near-zero vairance
#http://www.sthda.com/english/wiki/ggplot2-line-plot-quick-start-guide-r-software-and-data-visualization

zero.var = nearZeroVar(patients, saveMetrics=TRUE)
zero.var
write.table(zero.var, "Z:/HristinasFolder/20171101/zerovar-patients.txt", sep="\t")


```


```{r}

#GRADIENT BOOSTING ALGORITHM XGBOOST

## original imbalanced data sets: 
#train_patients_vars_imb
#test_patients_vars_imb


#1. FOR MANUAL TECHNIQUE
#format train file for model
train<-train_patients_vars
  
feature.names=names(train)

for (f in feature.names) {
  if (class(train[[f]])=="factor") {
    levels <- unique(c(train[[f]]))
    train[[f]] <- factor(train[[f]],
                   labels=make.names(levels))
  }
}
  
trainX<-train[,c(-1)]
trainY<-train[,1]

#format sampled test file for model
#test<-test_patients_vars
#format unsampled test file
test<-train_patients_vars_imb
  
feature.names=names(test)

for (f in feature.names) {
  if (class(test[[f]])=="factor") {
    levels <- unique(c(test[[f]]))
    test[[f]] <- factor(test[[f]],
                   labels=make.names(levels))
  }
}
  
testX<-test[,c(-1)]
testY<-test[,1]


xgbTrain = data.matrix(trainX)
xgbTest = data.matrix(testX)

trainY = as.integer(trainY)
testY = as.integer(testY)

numClasses = max(trainY)+1


###BEGIN_SECTION###BEGIN_SECTION###BEGIN_SECTION###BEGIN_SECTION######BEGIN_SECTION###BEGIN_SECTION######BEGIN_SECTION###BEGIN_SECTION######BEGIN_SECTION###BEGIN_SECTIO


#CROSS VALIDATION to find out the optumum number of rounds. We will pick the number of rounds where the test mlogloss is the least. 

param <- list("objective" = "multi:softprob",
              "eval_metric" = "merror",
              "num_class" = numClasses)


#The choice of "objective" = "multi:softprob" represents generalization of logistic link into multiple classes and returns a matrix of class probabilities, as opposed to "objective" = "multi:softmax" which returns the class of maximum probability.

#Sometimes it is important to use cross-validation to examine the model, for example, in order to find optimal number of iterations. In library xgboost this is done by function xgb.cv(). Fit boosting model.


set.seed(1234)
cv.nfold <- 5
cv.nround <- 300
bst.cv = xgb.cv(param=param, data = xgbTrain, label = trainY, nfold = cv.nfold, nrounds = cv.nround,prediction = T, verbose=T)
bst.cv; summary(bst.cv)

#calculate confidence interval for train logloss
CI(bst.cv$dt$train.merror.mean,.95)
#calculate confidence interval for test logloss
CI(bst.cv$dt$test.merror.mean,.95)

#Get confusion matrix for the cross-validation TRAINING SET set
pred.cv = matrix(bst.cv$pred, nrow=length(bst.cv$pred)/numClasses, ncol=numClasses)
pred.cv = max.col(pred.cv, "last")
confusionMatrix(factor(trainY+1), factor(pred.cv))

#not working great, cv.round coming out to past 800.
min_error = min(bst.cv$dt$test.merror.mean)
min_lerror_index = which.min(bst.cv$dt$test.merror.mean)
cv.round<-min_error_index

#use 150 rounds because that's where the merror stops decreasing significantly
######################################################################################################################################################################


###BEGIN_SECTION###BEGIN_SECTION###BEGIN_SECTION###BEGIN_SECTION######BEGIN_SECTION###BEGIN_SECTION######BEGIN_SECTION###BEGIN_SECTION######BEGIN_SECTION###BEGIN_SECTIO
  
#Arnab suggestion of changing the test and train proportions of the data (the folds) and seeing how the error and logloss vary
param <- list("objective" = "multi:softprob",
              "eval_metric" = "merror",
                "max_depth" = "5",
              "num_class" = numClasses)

results<-data.frame(percent_test=double(),
                 train.error=double(),
                 test.error=double())

i=2
cv.nfold=i
cv.nround=150
while (i<=10){
  bst.cv = xgb.cv(param=param, data = xgbTrain, label = trainY, nfold = cv.nfold, nrounds = cv.nround,prediction = T, verbose=T)
  results[i,1]<-1/i
  results[i,2]<-mean(bst.cv$dt$train.merror.mean)
  results[i,3]<-mean(bst.cv$dt$test.merror.mean)
  i=i+1
}

results
results<-results[-1,]

#results_4_trees<-results

library(ggplot2)
df <- data.frame(x=rep(results$percent_test,2), y=c(results$train.error, results$test.error), class=c(rep("train error", (nrow(results))), rep("test error",(nrow(results)))))

ggplot(df, aes(x=x, y=y, color=class)) + geom_line()+ labs(x = "Proportion Test",y="Error")

#End Arnab exploration of error
###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION


###BEGIN_SECTION###BEGIN_SECTION###BEGIN_SECTION###BEGIN_SECTION######BEGIN_SECTION###BEGIN_SECTION######BEGIN_SECTION###BEGIN_SECTION######BEGIN_SECTION###BEGIN_SECTIO

#Predict using the xgboost model with optimal cv.nrounds found above

#Use mlogloss as eval metric
set.seed(1234)
param <- list("objective" = "multi:softprob",
              "eval_metric" = "mlogloss",
              "max_depth" = "4",
              "num_class" = numClasses)

cv.nround = 150
#use 150 rounds because that's where the merror stops decreasing significantly
bst = xgboost(param=param, data = xgbTrain, label = trainY, nrounds=cv.nround,verbose=F)
pred.xgboost.man <- matrix(predict(bst, xgbTest), ncol = numClasses, byrow = TRUE)

## 1. METRICS FOR XGBOOST ALGORITHM FOR MANUAL TECHNIQUE

pred.xgboost.man<-pred.xgboost.man[,2]

pred.xgboost.man.logloss<-pred.xgboost.man
#use .5 as cutoff for categorization
pred.xgboost.man[pred.xgboost.man>=0.5]=1
pred.xgboost.man[pred.xgboost.man<0.5]=0

#get Confusion matrix and statistics
confusionMatrix(factor(testY), factor(pred.xgboost.man))

#confusion matrix
man_samp_xgboost<-table(testY,pred.xgboost.man);man_samp_xgboost
#confusion proportion table
round(prop.table(table(testY,pred.xgboost.man),1),2)
#miscategorization rate
man_xgboost_samp_miscat<-(man_samp_xgboost[1,2]+man_samp_xgboost[2,1])/(man_samp_xgboost[1,2]+man_samp_xgboost[1,1]+man_samp_xgboost[2,1]+man_samp_xgboost[2,2]);man_xgboost_samp_miscat

#calculation logloss
LogLoss <- function(actual, predicted, eps=0.00001) {
  predicted <- pmin(pmax(predicted, eps), 1-eps)
  -1/length(actual)*(sum(actual*log(predicted)+(1-actual)*log(1-predicted)))
}

man_logloss_xgboost<-LogLoss(testY, pred.xgboost.man.logloss); man_logloss_xgboost


###VISUALIZE CONFUSION MATRIX

library(caret)
# calculate the confusion matrix

cm <- confusionMatrix(data = pred.xgboost.man, reference = testY)

draw_confusion_matrix <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='gray30')
  text(195, 435, 'No HCV', cex=1.5)
  rect(250, 430, 340, 370, col='red4')
  text(295, 435, 'Has HCV', cex=1.5)
  text(125, 370, 'Predicted', cex=1.9, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.9, font=2)
  rect(150, 305, 240, 365, col='red4')
  rect(250, 305, 340, 365, col='gray30')
  text(140, 400, 'No HCV', cex=1.5, srt=90)
  text(140, 335, 'Has HCV', cex=1.5, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=3.0, font=2, col='white')
  text(195, 335, res[2], cex=3.0, font=2, col='white')
  text(295, 400, res[3], cex=3.0, font=2, col='white')
  text(295, 335, res[4], cex=3.0, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n', cex = 1.8)
  text(10, 85, names(cm$byClass[1]), cex=1.8, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 4), cex=1.8)
  text(30, 85, names(cm$byClass[2]), cex=1.8, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 4), cex=1.8)
  text(50, 85, names(cm$byClass[5]), cex=1.8, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 4), cex=1.8)
  text(70, 85, names(cm$byClass[6]), cex=1.8, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 4), cex=1.8)
  text(90, 85, names(cm$byClass[7]), cex=1.8, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 4), cex=1.8)

  # add in the accuracy information 
  text(10, 35, names(cm$overall[1]), cex=1.8, font=2)
  text(10, 20, round(as.numeric(cm$overall[1]), 4), cex=1.8)
  text(30, 35, names(cm$overall[3]), cex=1.8, font=2)
  text(30, 20, round(as.numeric(cm$overall[3]), 4), cex=1.8)
  text(50, 35, names(cm$overall[4]), cex=1.8, font=2)
  text(50, 20, round(as.numeric(cm$overall[4]), 4), cex=1.8)
  text(70, 35, names(cm$overall[2]), cex=1.8, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 4), cex=1.8)
}

draw_confusion_matrix(cm)






#Variable importance
man_var_imp<-xgb.importance(colnames(trainY), model = bst)


names <- dimnames(data.matrix(testX[,]))[[2]]
importance_matrix <- xgb.importance(names, model = bst)
#graph the varialbe importance
barplot(importance_matrix$Gain)

write.table(importance_matrix, "Z:/HristinasFolder/20171126/var_imp_12.txt", sep="\t")

library(ggplot2)
p <-ggplot(importance_matrix[1:30], aes(x=reorder(Feature,-Gain), y = Gain))
p +geom_bar(stat = "identity") +
xlab("Feature") + ylab("Gain") +
  ggtitle("Feature Importance-Top30") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))







#Draw ROC curve
library(pROC)
plot.roc(testY, pred.xgboost.man, print.auc=T, print.auc.y=0.5)


#Get "improved ROC plot"
library(pROC)
rocobj <- plot.roc(testY, pred.xgboost.man,
                main="ROC", percent=TRUE,
                ci=TRUE, # compute AUC (of AUC by default)
                print.auc=TRUE) # print the AUC (will contain the CI)
ciobj <- ci.se(rocobj, # CI of sensitivity
               specificities=seq(0, 100, 5)) # over a select set of specificities
plot(ciobj, type="shape", col="#1c61b6AA") # plot as a blue shape
plot(ci(rocobj, of="thresholds", thresholds="best")) # add one threshold



### GEt Corrected ROC plot

library(pROC)
plot.roc(testY, pred.xgboost.man, # data
         percent=TRUE, # show all values in percent
         partial.auc=c(100, 90), partial.auc.correct=TRUE, # define a partial AUC (pAUC)
         print.auc=TRUE, #display pAUC value on the plot with following options:
         print.auc.pattern="Corrected pAUC (100-90%% SP):\n%.1f%%", print.auc.col="#1c61b6",
         auc.polygon=TRUE, auc.polygon.col="#1c61b6", # show pAUC as a polygon
         max.auc.polygon=TRUE, max.auc.polygon.col="#1c61b622", # also show the 100% polygon
         main="Partial AUC (pAUC)")
plot.roc(testY, pred.xgboost.man,
         percent=TRUE, add=TRUE, type="n", # add to plot, but don't re-add the ROC itself (useless)
         partial.auc=c(100, 90), partial.auc.correct=TRUE,
         partial.auc.focus="se", # focus pAUC on the sensitivity
         print.auc=TRUE, print.auc.pattern="Corrected pAUC (100-90%% SE):\n%.1f%%", print.auc.col="#008600",
         print.auc.y=40, # do not print auc over the previous one
         auc.polygon=TRUE, auc.polygon.col="#008600",
         max.auc.polygon=TRUE, max.auc.polygon.col="#00860022")


###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION###END_SECTION


#work on odds ratio
library(epitools)

#convert the categorical variables to dummy variables first, and then do odds ratios
vars<-c("AVERAGE_ENC_TYPE_101", "AVERAGE_ENC_TYPE_117", "Avg_Num_Encounters_yr", "AVERAGE_ENC_TYPE_203", "AVERAGE_ENC_TYPE_60", "AVERAGE_ENC_TYPE_105", "AVERAGE_MED_ANALGESICS", "AVERAGE_ENC_TYPE_70", "AVERAGE_ICD_0V72", "AVERAGE_MED_ANTIARTHRITICS", "LANGUAGE", "AVERAGE_MED_GASTROINTESTINAL", "GENERATION_CATEGORY.x","AVE_COMB_ANESTHESIA", "AVERAGE_MED_ANTIBIOTICS", "AVERAGE_ICD_0305","AGE_CATEGORY","AVERAGE_MED_ELECT.CALORIC.H2O", "AVERAGE_ICD_0V76","RACE","AVERAGE_MED_VITAMINS","ETHNICITY","AVERAGE_MED_ANTIHISTAMINES","AVERAGE_ENC_TYPE_107","AVERAGE_MED_CARDIOVASCULAR")

results_odds$vars<-as.data.frame[vars,1]

train_patients_vars$var_bin<-scale(train_patients_vars$AVERAGE_MED_ANALGESICS)
train_patients_vars$var_bin<-cut(train_patients_vars$var_bin,7)



ratios<-oddsratio.fisher(train_patients_vars$var_bin, y = train_patients_vars$HCV_STATUS,
conf.level = 0.95,
rev = c("neither", "rows", "columns", "both"),
correction = FALSE,
verbose = FALSE)




```




```{r}

#2. FOR UNDERSAMPING TECHNIQUE

train<-data_balanced_under
  
feature.names=names(train)

for (f in feature.names) {
  if (class(train[[f]])=="factor") {
    levels <- unique(c(train[[f]]))
    train[[f]] <- factor(train[[f]],
                   labels=make.names(levels))
  }
}
  
trainX<-train[,c(-1)]
trainY<-train[,1]

test<-test_balanced_under
  
feature.names=names(test)

for (f in feature.names) {
  if (class(test[[f]])=="factor") {
    levels <- unique(c(test[[f]]))
    test[[f]] <- factor(test[[f]],
                   labels=make.names(levels))
  }
}
  
testX<-test[,c(-1)]
testY<-test[,1]


xgbTrain = data.matrix(trainX)
xgbTest = data.matrix(testX)

trainY = as.integer(trainY)
testY = as.integer(testY)

numClasses = max(trainY)+1
param <- list("objective" = "multi:softprob",
              "eval_metric" = "mlogloss",
              "num_class" = numClasses)


#The choice of "objective" = "multi:softprob" represents generalization of logistic link into multiple classes and returns a matrix of class probabilities, as opposed to "objective" = "multi:softmax" which returns the class of maximum probability.

#Sometimes it is important to use cross-validation to examine the model, for example, in order to find optimal number of iterations. In library xgboost this is done by function xgb.cv(). Fit boosting model.






#2. FOR OVERSAMPING TECHNIQUE

set.seed(567)
train<-data_balanced_over
  
feature.names=names(train)

for (f in feature.names) {
  if (class(train[[f]])=="factor") {
    levels <- unique(c(train[[f]]))
    train[[f]] <- factor(train[[f]],
                   labels=make.names(levels))
  }
}
  
trainX<-train[,c(-1)]
trainY<-train[,1]

test<-test_balanced_over
  
feature.names=names(test)

for (f in feature.names) {
  if (class(test[[f]])=="factor") {
    levels <- unique(c(test[[f]]))
    test[[f]] <- factor(test[[f]],
                   labels=make.names(levels))
  }
}
  
testX<-test[,c(-1)]
testY<-test[,1]


xgbTrain = data.matrix(trainX)
xgbTest = data.matrix(testX)
trainY = as.integer(trainY)
testY = as.integer(testY)

numClasses = max(trainY)+1
param <- list("objective" = "multi:softprob",
              "eval_metric" = "mlogloss",
              "num_class" = numClasses)


#The choice of "objective" = "multi:softprob" represents generalization of logistic link into multiple classes and returns a matrix of class probabilities, as opposed to "objective" = "multi:softmax" which returns the class of maximum probability.

#Sometimes it is important to use cross-validation to examine the model, for example, in order to find optimal number of iterations. In library xgboost this is done by function xgb.cv(). Fit boosting model.



#3. FOR BOTH SAMPLING TECHNIQUE

train<-data_balanced_both
  
feature.names=names(train)

for (f in feature.names) {
  if (class(train[[f]])=="factor") {
    levels <- unique(c(train[[f]]))
    train[[f]] <- factor(train[[f]],
                   labels=make.names(levels))
  }
}
  
trainX<-train[,c(-1)]
trainY<-train[,1]

test<-test_balanced_both
  
feature.names=names(test)

for (f in feature.names) {
  if (class(test[[f]])=="factor") {
    levels <- unique(c(test[[f]]))
    test[[f]] <- factor(test[[f]],
                   labels=make.names(levels))
  }
}
  
testX<-test[,c(-1)]
testY<-test[,1]


xgbTrain = data.matrix(trainX)
xgbTest = data.matrix(testX)
trainY = as.integer(trainY)
testY = as.integer(testY)

numClasses = max(trainY)+1
param <- list("objective" = "multi:softprob",
              "eval_metric" = "mlogloss",
              "num_class" = numClasses)


#The choice of "objective" = "multi:softprob" represents generalization of logistic link into multiple classes and returns a matrix of class probabilities, as opposed to "objective" = "multi:softmax" which returns the class of maximum probability.

#Sometimes it is important to use cross-validation to examine the model, for example, in order to find optimal number of iterations. In library xgboost this is done by function xgb.cv(). Fit boosting model.




```




```{r}

man_samp_false_positives<-cbind(testY,pred.xgboost.man)



```










```{r}
#######STOP HERE - BELOW CODE NOT CLEAN############

```



```{r}
#K Nearest neighbors
data("iris")
head(iris)
library(class)


#KNN on undersampled data
knn_under_train <- data_balanced_under[,2:560]
knn_under_test <- test_balanced_under[,2:560]
knn_under_trainLabels <- data_balanced_under[,1]
knn_under_testLabels <- test_balanced_under[,1]

knn_under_trainLabels <- as.factor(knn_under_trainLabels)
knn_under_testLabels <- as.factor(knn_under_testLabels)

knn_under <- knn(train=knn_under_train, test=knn_under_test, cl=knn_under_trainLabels, k=3)

```



```{r}

#######STOP HERE - BELOW CODE NOT CLEAN############

#SUPPORT VECTOR MACHINE - NOT YET WORKING
suppressWarnings(library(caret))
suppressWarnings(library(kernlab))
suppressWarnings(library(ROCR))

#Demographic variables = 4-7
#ICD codes grouped by HEDIS value sets = 8-63
#CPT codes by HEDIS value sets = 64-122
#Encounter variables = 123-129
#ACS variables = 130-171
#Social variables = 172-179
#ICD grouped by CCS variables = 180-460


#2. UNDER-SAMPLING FOR SVM
trainX<-data_balanced_both[,c(2:5)]
trainY<-data_balanced_both[,1]

set.seed(0)
ctrl <- trainControl(method = "cv", number = 5, classProbs =  TRUE)
svmFitIso <- train(trainX,trainY,
                  method = "svmRadial",
                  tuneGrid = data.frame(.C = c(1,2,4,8,16),.sigma = .001), 
                  trControl = ctrl,
                  preProc = c("center", "scale"))


```




```{r}
#######STOP HERE - BELOW CODE NOT CLEAN############


#It's time to evaluate the accuracy of respective predictions. Using inbuilt function roc.curve allows us to capture roc metric.

#AUC ROSE
#roc.curve(test_patients_vars_imb$HCV_STATUS, pred.tree.rose[,2])

#AUC Oversampling
#roc.curve(test_patients_vars_imb$HCV_STATUS, pred.tree.over[,2])

#AUC Undersampling
#roc.curve(test_patients_vars_imb$HCV_STATUS, pred.tree.under[,2])

#AUC Both
#roc.curve(test_patients_vars_imb$HCV_STATUS, pred.tree.both[,2])


```


